{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292cba28-4d43-486f-bf9f-68a8f2e74235",
   "metadata": {},
   "source": [
    "# 2. Training Our First Neural Network with PyTorch\n",
    "\n",
    "To train a neural network in PyTorch, you will first need to understand the job of a loss function. You will then realize that training a network requires minimizing that loss function, which is done by calculating gradients. You will learn how to use these gradients to update your model's parameters, and finally, you will write your first training loop.\n",
    "\n",
    "### Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2d5879-1698-4d9d-b489-69544429677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import expectexception\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc755153-6d80-4ad3-b8cf-8bfd73965528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab20ca-ed18-472e-9f5a-1d88b38c739f",
   "metadata": {},
   "source": [
    "## 2.1 Running a forward pass\n",
    "\n",
    "### Binary classification: forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379b6d8a-c623-4719-bbd8-2785f35a6e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4032],\n",
      "        [0.3973],\n",
      "        [0.3619],\n",
      "        [0.3240],\n",
      "        [0.3497]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create input data of shape 5x6\n",
    "input_data = torch.tensor([\n",
    "    [-0.4421, 1.5207, 2.0607, -0.3647, 0.4691, 0.0946],\n",
    "    [-0.9155, -0.0475, -1.3645, 0.6336, -1.9520, -0.3398],\n",
    "    [ 0.7406, 1.6763, -0.8511, 0.2432, 0.1123, -0.0633],\n",
    "    [-1.6630, -0.0718, -0.1285, 0.5396, -0.0288, -0.8622],\n",
    "    [-0.7413, 1.7920, -0.0883, -0.6685, 0.4745, -0.4245]\n",
    "])\n",
    "\n",
    "# Create binary classification model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4), # First linear layer\n",
    "    nn.Linear(4, 1), # Second linear layer\n",
    "    nn.Sigmoid() # Sigmoid activation function\n",
    ")\n",
    "\n",
    "# Pass input data through model\n",
    "output = model(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f6459-44b8-4235-a186-a97c43c06178",
   "metadata": {},
   "source": [
    "### Multi-class classification: forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88c0cf1-ba16-4dc9-bd9c-94eac7bcb9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1036, 0.4562, 0.4402],\n",
      "        [0.3329, 0.4356, 0.2315],\n",
      "        [0.1878, 0.6083, 0.2038],\n",
      "        [0.1488, 0.4402, 0.4109],\n",
      "        [0.1261, 0.5277, 0.3462]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create input data of shape 5x6\n",
    "input_data = torch.tensor([\n",
    "    [-0.4421, 1.5207, 2.0607, -0.3647, 0.4691, 0.0946],\n",
    "    [-0.9155, -0.0475, -1.3645, 0.6336, -1.9520, -0.3398],\n",
    "    [ 0.7406, 1.6763, -0.8511, 0.2432, 0.1123, -0.0633],\n",
    "    [-1.6630, -0.0718, -0.1285, 0.5396, -0.0288, -0.8622],\n",
    "    [-0.7413, 1.7920, -0.0883, -0.6685, 0.4745, -0.4245]\n",
    "])\n",
    "\n",
    "# Specify model has three classes\n",
    "n_classes = 3\n",
    "\n",
    "# Create multiclass classification model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4), # First linear layer\n",
    "    nn.Linear(4, n_classes), # Second linear layer\n",
    "    nn.Softmax(dim=-1) # Softmax activation\n",
    ")\n",
    "\n",
    "# Pass input data through model\n",
    "output = model(input_data)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40f9f9-1768-437b-8381-f31bdf68d4e3",
   "metadata": {},
   "source": [
    "### Ex.1 - Building a binary classifier in PyTorch\n",
    "Recall that a small neural network with a single linear layer followed by a sigmoid function is a binary classifier. It acts just like a logistic regression.\n",
    "\n",
    "In this exercise, you'll practice building this small network and interpreting the output of the classifier.\n",
    "\n",
    "The torch package and the torch.nn package have already been imported for you.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Create a neural network that takes a tensor of dimensions 1x8 as input, and returns an output of the correct shape for binary classification.\n",
    "2. Pass the output of the linear layer to a sigmoid, which both takes in and return a single float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8504648a-d955-4af4-92a1-a53d362ed70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6407]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8, 1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f738ecd-61da-4ff6-b3a1-887c5ad38896",
   "metadata": {},
   "source": [
    "### Ex.2 - From regression to multi-class classification\n",
    "Recall that the models we have seen for binary classification, multi-class classification and regression have all been similar, barring a few tweaks to the model.\n",
    "\n",
    "In this exercise, you'll start by building a model for regression, and then tweak the model to perform a multi-class classification.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a neural network with exactly four linear layers, which takes the input tensor as input, and outputs a regression value, using any shapes you like for the hidden layers.\n",
    "2. A similar neural network to the one you just built is provided, containing four linear layers; update this network to perform a multi-class classification with four outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13234742-f143-488c-9e34-47224f3ed918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1565]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network with exactly four linear layers, which takes the input tensor as input,\n",
    "# and outputs a regression value, using any shapes you like for the hidden layers.\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 20),\n",
    "    nn.Linear(20, 12),\n",
    "    nn.Linear(12, 8),\n",
    "    nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b04bf991-c820-43ea-8f96-62beb8f2dfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2372, 0.2076, 0.3518, 0.2034]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# A similar neural network to the one you just built is provided, containing four linear layers;\n",
    "# update this network to perform a multi-class classification with four outputs.\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 20),\n",
    "    nn.Linear(20, 12),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.Linear(6, 4), \n",
    "    nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b7c68-eb12-464e-b2d6-e413edc9f81a",
   "metadata": {},
   "source": [
    "## 2.2 Using loss functions to assess model predictions\n",
    "\n",
    "### One-hot encoding concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828a1ab3-91a2-4e07-91f5-4508e9010f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_numpy = np.array([1, 0, 0])\n",
    "one_hot_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77711f90-0661-40a1-bc90-ec30ef1c962b",
   "metadata": {},
   "source": [
    "### Transforming labels with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8271680d-1b27-49b5-9c13-c8ce4aa286d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(0), num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a81996e3-75f3-42cf-9288-1c7dbfa1ed96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(1), num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51b8f9bb-6434-4779-bb3f-f56f5225df99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(2), num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb621d-4047-4a7b-9614-88496bb6deeb",
   "metadata": {},
   "source": [
    "### Cross entropy loss in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b5fe79f-ec9e-413e-92b9-11e4bc277a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n",
      "\u001b[0;32m      3\u001b[0m one_hot_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[0;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n",
      "\u001b[1;32m----> 6\u001b[0m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_target\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\deep\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n",
      "\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[1;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\deep\\lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n",
      "\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n",
      "\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long\n"
     ]
    }
   ],
   "source": [
    "%%expect_exception RuntimeError\n",
    "\n",
    "# Integers are not allowed in `CrossEntropyLoss`\n",
    "scores = torch.tensor([[-0.1211, 0.1059]])\n",
    "one_hot_target = torch.tensor([[1, 0]])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "criterion(scores, one_hot_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b656d67-a661-4d0f-931a-a85f4f7e72fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8131, dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Casting all values to double\n",
    "scores = torch.tensor([[-0.1211, 0.1059]])\n",
    "one_hot_target = torch.tensor([[1, 0]])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "criterion(scores.double(), one_hot_target.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50c46108-c364-4866-b616-9a2611f97cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8131)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the beginning only float values\n",
    "scores = torch.tensor([[-0.1211, 0.1059]])\n",
    "one_hot_target = torch.tensor([[1.0, 0.0]])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "criterion(scores, one_hot_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103a956-c807-4c1f-b4cf-493824b44d82",
   "metadata": {},
   "source": [
    "### Ex.3 - Creating one-hot encoded labels\n",
    "\n",
    "One-hot encoding is a technique that turns a single integer label into a vector of N elements, where N is the number of classes in your dataset. This vector only contains zeros and ones. In this exercise, you'll create the one-hot encoded vector of the label y provided.\n",
    "\n",
    "You'll practice doing this manually, and then make your life easier by leveraging the help of PyTorch! Your dataset contains three classes.\n",
    "\n",
    "NumPy is already imported as np, and torch.nn.functional as F. The torch package is also imported.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Manually create a one-hot encoded vector of the ground truth label y by filling in the NumPy array provided.\n",
    "2. Create a one-hot encoded vector of the ground truth label y using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f654d3c4-a5e9-449a-a14f-a91f147e4e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n",
      "tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "print(one_hot_numpy)\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(1), num_classes=3)\n",
    "print(one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5118311-f4d6-4e9d-b808-c657f3d48f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_pytorch == one_hot_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de18626-44c3-48f1-82f1-08db99ab40cb",
   "metadata": {},
   "source": [
    "### Ex.4 - Calculating cross entropy loss\n",
    "\n",
    "Cross entropy loss is the most used loss for classification problems. In this exercise, you will create inputs and calculate cross entropy loss in PyTorch. You are provided with the ground truth label y and a vector of scores predicted by your model.\n",
    "\n",
    "You'll start by creating a one-hot encoded vector of the ground truth label y, which is a required step to compare y with the scores predicted by your model. Next, you'll create a cross entropy loss function. Last, you'll call the loss function, which takes scores (model predictions before the final softmax function), and the one-hot encoded ground truth label, as inputs. It outputs a single float, the loss of that sample.\n",
    "\n",
    "torch, CrossEntropyLoss, and torch.nn.functional as F have already been imported for you.\n",
    "\n",
    "**Instructions**\n",
    "    \n",
    "1. Create the one-hot encoded vector of the ground truth label y and assign it to one_hot_label.\n",
    "2. Create the cross entropy loss function and store it as criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8b19656-596a-4617-a187-9ccdb87f61fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: tensor([[ 0.1000,  6.0000, -2.0000,  3.2000]])\n",
      "One Hot Label: tensor([[0, 0, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "print('Scores:', scores)\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), num_classes=scores.shape[1])\n",
    "print('One Hot Label:', one_hot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba8176f8-128b-4432-aff8-23e16f0d7327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c65026d-6584-4b5d-b236-1551e3b1b744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Create the cross entropy loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2c3b7-5ae7-44b9-99e1-74ed53982bbe",
   "metadata": {},
   "source": [
    "## 2.3 Using derivatives to update model parameters\n",
    "\n",
    "### Backpropagation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c75e767-2b40-41ce-9df7-36e1fa163247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: tensor([[ 0.8796, -1.3361, -0.4426,  0.8302, -0.4967,  1.1242,  1.1429, -0.0919,\n",
      "         -0.5407,  0.1484, -0.2874,  0.2823,  0.2772, -0.2768,  0.0566,  0.0611]])\n",
      "Target: tensor([[1, 0]])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.randn(1, 16)\n",
    "target = F.one_hot(torch.tensor([0]), num_classes=2)\n",
    "print('Sample:', sample)\n",
    "print('Target:', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f73eeeb-aa05-4738-a13d-33f144ef03b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tensor([[0.2020, 0.3538]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Create the model and run a forward pass\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.Linear(4, 2)\n",
    ")\n",
    "\n",
    "prediction = model(sample)\n",
    "print('Prediction:', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9456f0de-1abe-47b7-a44c-635432d6c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.7719, dtype=torch.float64, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss and compute the gradients\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(prediction.double(), target.double())\n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d8d6227-6c5d-492e-87be-99ed775d23fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.7719, dtype=torch.float64, grad_fn=<DivBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0239, -0.0363, -0.0120,  0.0226, -0.0135,  0.0306,  0.0311, -0.0025,\n",
       "          -0.0147,  0.0040, -0.0078,  0.0077,  0.0075, -0.0075,  0.0015,  0.0017],\n",
       "         [ 0.0241, -0.0365, -0.0121,  0.0227, -0.0136,  0.0307,  0.0313, -0.0025,\n",
       "          -0.0148,  0.0041, -0.0079,  0.0077,  0.0076, -0.0076,  0.0015,  0.0017],\n",
       "         [ 0.0912, -0.1385, -0.0459,  0.0861, -0.0515,  0.1165,  0.1185, -0.0095,\n",
       "          -0.0560,  0.0154, -0.0298,  0.0293,  0.0287, -0.0287,  0.0059,  0.0063],\n",
       "         [ 0.0268, -0.0407, -0.0135,  0.0253, -0.0151,  0.0342,  0.0348, -0.0028,\n",
       "          -0.0165,  0.0045, -0.0088,  0.0086,  0.0084, -0.0084,  0.0017,  0.0019],\n",
       "         [ 0.0432, -0.0656, -0.0217,  0.0408, -0.0244,  0.0552,  0.0561, -0.0045,\n",
       "          -0.0266,  0.0073, -0.0141,  0.0139,  0.0136, -0.0136,  0.0028,  0.0030],\n",
       "         [ 0.0960, -0.1459, -0.0483,  0.0906, -0.0542,  0.1228,  0.1248, -0.0100,\n",
       "          -0.0590,  0.0162, -0.0314,  0.0308,  0.0303, -0.0302,  0.0062,  0.0067],\n",
       "         [-0.0087,  0.0132,  0.0044, -0.0082,  0.0049, -0.0111, -0.0113,  0.0009,\n",
       "           0.0053, -0.0015,  0.0028, -0.0028, -0.0027,  0.0027, -0.0006, -0.0006],\n",
       "         [-0.0457,  0.0694,  0.0230, -0.0431,  0.0258, -0.0584, -0.0594,  0.0048,\n",
       "           0.0281, -0.0077,  0.0149, -0.0147, -0.0144,  0.0144, -0.0029, -0.0032]]),\n",
       " tensor([ 0.0272,  0.0273,  0.1037,  0.0305,  0.0491,  0.1092, -0.0099, -0.0520]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "print('Loss:', loss)\n",
    "\n",
    "# Access each layer's gradients\n",
    "model[0].weight.grad, model[0].bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95c6b90c-8d3c-4dd6-b15d-3ff48fd99550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0239, -0.0363, -0.0120,  0.0226, -0.0135,  0.0306,  0.0311, -0.0025,\n",
       "         -0.0147,  0.0040, -0.0078,  0.0077,  0.0075, -0.0075,  0.0015,  0.0017],\n",
       "        [ 0.0241, -0.0365, -0.0121,  0.0227, -0.0136,  0.0307,  0.0313, -0.0025,\n",
       "         -0.0148,  0.0041, -0.0079,  0.0077,  0.0076, -0.0076,  0.0015,  0.0017],\n",
       "        [ 0.0912, -0.1385, -0.0459,  0.0861, -0.0515,  0.1165,  0.1185, -0.0095,\n",
       "         -0.0560,  0.0154, -0.0298,  0.0293,  0.0287, -0.0287,  0.0059,  0.0063],\n",
       "        [ 0.0268, -0.0407, -0.0135,  0.0253, -0.0151,  0.0342,  0.0348, -0.0028,\n",
       "         -0.0165,  0.0045, -0.0088,  0.0086,  0.0084, -0.0084,  0.0017,  0.0019],\n",
       "        [ 0.0432, -0.0656, -0.0217,  0.0408, -0.0244,  0.0552,  0.0561, -0.0045,\n",
       "         -0.0266,  0.0073, -0.0141,  0.0139,  0.0136, -0.0136,  0.0028,  0.0030],\n",
       "        [ 0.0960, -0.1459, -0.0483,  0.0906, -0.0542,  0.1228,  0.1248, -0.0100,\n",
       "         -0.0590,  0.0162, -0.0314,  0.0308,  0.0303, -0.0302,  0.0062,  0.0067],\n",
       "        [-0.0087,  0.0132,  0.0044, -0.0082,  0.0049, -0.0111, -0.0113,  0.0009,\n",
       "          0.0053, -0.0015,  0.0028, -0.0028, -0.0027,  0.0027, -0.0006, -0.0006],\n",
       "        [-0.0457,  0.0694,  0.0230, -0.0431,  0.0258, -0.0584, -0.0594,  0.0048,\n",
       "          0.0281, -0.0077,  0.0149, -0.0147, -0.0144,  0.0144, -0.0029, -0.0032]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = model[0].weight\n",
    "weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c546e282-6fca-42de-af08-57866856b97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0349, -0.0738, -0.2834, -0.2535,  0.1177, -0.0723, -0.0695,  0.0597],\n",
       "         [ 0.0163,  0.0346,  0.1329,  0.1189, -0.0552,  0.0339,  0.0326, -0.0280],\n",
       "         [ 0.0359,  0.0760,  0.2919,  0.2611, -0.1212,  0.0745,  0.0716, -0.0615],\n",
       "         [-0.0160, -0.0339, -0.1300, -0.1163,  0.0540, -0.0332, -0.0319,  0.0274]]),\n",
       " tensor([-0.3511,  0.1646,  0.3616, -0.1611]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[1].weight.grad, model[1].bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f5b62c1-6941-4cf4-860a-a6e80c2663f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0043, -0.0242,  0.0798,  0.1621],\n",
       "         [-0.0043,  0.0242, -0.0798, -0.1621]]),\n",
       " tensor([-0.5379,  0.5379]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[2].weight.grad, model[2].bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2db26e-95b0-4a39-a04d-686ea20df27b",
   "metadata": {},
   "source": [
    "### Updating model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c759638-0613-4b40-8c5a-c835b87c0fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: tensor([[ 0.1911,  0.2075, -0.0586,  0.2296, -0.0548,  0.0504, -0.1217,  0.1468,\n",
      "          0.2204, -0.1834,  0.2173,  0.0468,  0.1847,  0.0339,  0.1205, -0.0353],\n",
      "        [ 0.1927,  0.0370, -0.1167,  0.0637, -0.1152, -0.0293, -0.1016,  0.1658,\n",
      "         -0.1973, -0.1153, -0.0706, -0.1503,  0.0236, -0.2469,  0.2258, -0.2124],\n",
      "        [ 0.1929,  0.0417, -0.0811,  0.1544,  0.0390,  0.2019,  0.0272, -0.0788,\n",
      "          0.0672, -0.0678,  0.1052,  0.2232,  0.1445, -0.1093,  0.1443,  0.0447],\n",
      "        [ 0.1269, -0.1523, -0.2475, -0.0966, -0.1917,  0.2051,  0.0720,  0.1036,\n",
      "          0.0791, -0.0044,  0.1957, -0.1776,  0.0157, -0.1706,  0.0771, -0.0861],\n",
      "        [ 0.0766, -0.0520,  0.2074, -0.1482, -0.1491, -0.1492,  0.2248,  0.0833,\n",
      "          0.2406, -0.2063, -0.2480, -0.1956, -0.1682,  0.1013,  0.0895,  0.2077],\n",
      "        [-0.1292, -0.1703,  0.1327, -0.1011,  0.1518, -0.0594,  0.1429, -0.1942,\n",
      "         -0.1261,  0.0762,  0.0529, -0.0638,  0.1490,  0.1700, -0.1813, -0.1335],\n",
      "        [ 0.2289, -0.0844, -0.0886, -0.2419, -0.1432,  0.0625, -0.0330, -0.1815,\n",
      "          0.0059, -0.1708, -0.2121, -0.1377, -0.2188, -0.1592,  0.2499,  0.0472],\n",
      "        [ 0.0771, -0.2332, -0.1642, -0.0832,  0.0391, -0.2199, -0.1077, -0.1497,\n",
      "          0.0007, -0.0930, -0.0173, -0.1694, -0.1716, -0.1459, -0.0856, -0.1973]],\n",
      "       grad_fn=<SubBackward0>)\n",
      "Bias: tensor([ 0.2096, -0.0496,  0.2150,  0.0779, -0.2117,  0.1729, -0.0688, -0.0958],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Learning rate is typically small\n",
    "lr = 0.001\n",
    "\n",
    "# Update the weights\n",
    "weight = model[0].weight\n",
    "weight_grad = model[0].weight.grad\n",
    "weight = weight - lr * weight_grad\n",
    "\n",
    "# Update the biases\n",
    "bias = model[0].bias\n",
    "bias_grad = model[0].bias.grad\n",
    "bias = bias - lr * bias_grad\n",
    "\n",
    "print('Weight:', weight)\n",
    "print('Bias:', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148a307a-98b7-4ef1-a6c2-6a8f4662c83d",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "268895b8-289e-488f-9503-f687db878638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc4c506f-8c61-4950-a3af-a330c75ad3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer handles updating model parameters (or weights) after calculation of local\n",
    "# gradients\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a6e91e-491a-4e89-a4f1-e3d74f6ab09f",
   "metadata": {},
   "source": [
    "### Ex.5 - Estimating a sample\n",
    "In previous exercises, you used linear layers to build networks.\n",
    "\n",
    "Recall that the operation performed by `nn.Linear()` is to take an input $ X $ and apply the transformation $ W * X + b $, where $ W $ and $ b $ are two tensors (called the weight and bias).\n",
    "\n",
    "A critical part of training PyTorch models is to calculate gradients of the weight and bias tensors with respect to a loss function.\n",
    "\n",
    "In this exercise, you will calculate weight and bias tensor gradients using cross entropy loss and a sample of data.\n",
    "\n",
    "The following tensors are provided:\n",
    "\n",
    "- weight: a $ 2 \\times 9 $ -element tensor\n",
    "- bias: a $ 2 $ -element tensor\n",
    "- preds: a $ 1 \\times 2 $ -element tensor containing the model predictions\n",
    "- target: a $ 1 \\times 2 $ -element one-hot encoded tensor containing the ground-truth label\n",
    "\n",
    "Note: If you see the error RuntimeError: Trying to backward through the graph a second time, please reload the page and try again.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Use the criterion you have defined to calculate the loss value with respect to the predictions and target values.\n",
    "2. Compute the gradients of the cross entropy loss.\n",
    "3. Display the gradients of the weight and bias tensors, in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d898c412-7843-4886-92ec-075be54da777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: tensor([[-0.0662, -0.4235, -2.3768,  0.0641, -0.3435,  1.2287, -0.2754, -0.2109,\n",
      "          0.9287]])\n",
      "Target: tensor([[1, 0]])\n",
      "Preds: tensor([[-0.6585, -0.3968]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Model declaration\n",
    "sample = torch.randn(1, 9)\n",
    "target = F.one_hot(torch.tensor([0]), num_classes=2)\n",
    "print('Sample:', sample)\n",
    "print('Target:', target)\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Create the model and run a forward pass\n",
    "# del model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(9, 2),\n",
    "    nn.Linear(2, 2)\n",
    ")\n",
    "\n",
    "preds = model(sample)\n",
    "print('Preds:', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03b08e3f-546d-405d-820e-701d7e815a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: Parameter containing:\n",
      "tensor([[ 0.2548,  0.2767, -0.0781,  0.3062, -0.0730,  0.0673, -0.1623,  0.1958,\n",
      "          0.2938],\n",
      "        [-0.2445,  0.2897,  0.0624,  0.2463,  0.0451,  0.1607, -0.0471,  0.2570,\n",
      "          0.0493]], requires_grad=True)\n",
      "Bias: Parameter containing:\n",
      "tensor([-0.1556,  0.0850], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Getting the weight and bias\n",
    "weight = model[0].weight\n",
    "bias = model[0].bias\n",
    "\n",
    "print('Weight:', weight)\n",
    "print('Bias:', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff79641e-f10b-4756-ab40-f128d0a5539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.8325, dtype=torch.float64, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss and compute the gradients\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(preds.double(), target.double())\n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28d28155-03a4-485b-afa3-bef59dfbba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0014, -0.0092, -0.0518,  0.0014, -0.0075,  0.0268, -0.0060, -0.0046,\n",
      "          0.0203],\n",
      "        [-0.0207, -0.1321, -0.7414,  0.0200, -0.1071,  0.3832, -0.0859, -0.0658,\n",
      "          0.2897]])\n",
      "tensor([0.0218, 0.3119])\n"
     ]
    }
   ],
   "source": [
    "# Compute the gradients of the loss\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "print(model[0].weight.grad)\n",
    "print(model[0].bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3fa641-3da5-4438-8007-5aca17e3833f",
   "metadata": {},
   "source": [
    "### Ex.6 - Accessing the model parameters\n",
    "A PyTorch model created with the nn.Sequential() is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different linear layers of a neural network. You won't be accessing the sigmoid.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Access the weight parameter of the first linear layer.\n",
    "2. Access the bias parameter of the second linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1510737a-a5c2-4c57-aa40-de0d853fa3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of first layer: Parameter containing:\n",
      "tensor([[-0.0706, -0.1503,  0.0236, -0.2469,  0.2258, -0.2124,  0.1930,  0.0416,\n",
      "         -0.0812,  0.1545,  0.0390,  0.2020,  0.0273, -0.0788,  0.0672, -0.0678],\n",
      "        [ 0.1052,  0.2232,  0.1445, -0.1093,  0.1443,  0.0447,  0.1270, -0.1524,\n",
      "         -0.2475, -0.0966, -0.1918,  0.2051,  0.0720,  0.1036,  0.0791, -0.0043],\n",
      "        [ 0.1957, -0.1776,  0.0157, -0.1706,  0.0771, -0.0861,  0.0766, -0.0521,\n",
      "          0.2073, -0.1482, -0.1491, -0.1491,  0.2249,  0.0833,  0.2406, -0.2063],\n",
      "        [-0.2480, -0.1956, -0.1682,  0.1013,  0.0895,  0.2077, -0.1291, -0.1704,\n",
      "          0.1326, -0.1011,  0.1517, -0.0593,  0.1430, -0.1942, -0.1262,  0.0762],\n",
      "        [ 0.0529, -0.0637,  0.1490,  0.1700, -0.1813, -0.1335,  0.2289, -0.0844,\n",
      "         -0.0886, -0.2419, -0.1432,  0.0625, -0.0330, -0.1815,  0.0059, -0.1708],\n",
      "        [-0.2121, -0.1377, -0.2188, -0.1592,  0.2499,  0.0472,  0.0770, -0.2332,\n",
      "         -0.1642, -0.0832,  0.0391, -0.2200, -0.1077, -0.1497,  0.0007, -0.0930],\n",
      "        [-0.0173, -0.1694, -0.1716, -0.1459, -0.0856, -0.1973,  0.2096, -0.0496,\n",
      "          0.2151,  0.0779, -0.2117,  0.1730, -0.0688, -0.0958, -0.2075, -0.2485],\n",
      "        [ 0.0715, -0.0546,  0.0973, -0.2052,  0.1856, -0.1835, -0.0432,  0.0522,\n",
      "          0.1291,  0.2018,  0.2277, -0.1982,  0.0629, -0.1075, -0.0274, -0.1871]],\n",
      "       requires_grad=True)\n",
      "Bias of second layer: Parameter containing:\n",
      "tensor([ 0.2737, -0.3328], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(8, 2)\n",
    ")\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias\n",
    "\n",
    "print('Weight of first layer:', weight_0)\n",
    "print('Bias of second layer:', bias_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7c2f4-5564-4e2e-8831-54d2face4ef1",
   "metadata": {},
   "source": [
    "### Ex.7 - Updating the weights manually\n",
    "\n",
    "Now that you know how to access weights and biases, you will manually perform the job of the PyTorch optimizer. PyTorch functions can do what you're about to do, but it's helpful to do the work manually at least once, to understand what's going on under the hood.\n",
    "\n",
    "A neural network of three layers has been created and stored as the model variable. This network has been used for a forward pass and the loss and its derivatives have been calculated. A default learning rate, lr, has been chosen to scale the gradients when performing the update.\n",
    "\n",
    "**Instructions**\n",
    "1. Create the gradient variables by accessing the local gradients of each weight tensor.\n",
    "2. Update the weights using the gradients scaled by the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14d35cd3-8fb4-4b88-9f21-2c2f51e1615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lr: 0.001\n",
      "Sample: tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
      "          0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473]])\n",
      "Target: tensor([[1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "sample = torch.tensor([[\n",
    "    -1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
    "    0.3223, -1.2633,  0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473\n",
    "]])\n",
    "\n",
    "target = torch.tensor([[1., 0.]])\n",
    "\n",
    "print('Lr:', lr)\n",
    "print('Sample:', sample)\n",
    "print('Target:', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a21e1af6-73d0-42b1-ab77-633ceb9b7303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (1): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (2): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "Preds: tensor([[0.1740, 0.2616]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Setting the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.Linear(4, 2)\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Predictions\n",
    "preds = model(sample)\n",
    "print('Preds:', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc8f8e86-1554-4ee8-b892-6aa583460208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.7379, dtype=torch.float64, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss and compute the gradients\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(preds.double(), target.double())\n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d443709e-ece2-490a-a523-5b1b3dec7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradients of the loss\n",
    "loss.backward()\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = model[0].weight.grad\n",
    "grads1 = model[1].weight.grad\n",
    "grads2 = model[2].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99c805ab-8072-49af-86fd-3f2e30cb58e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1912,  0.2075, -0.0586,  0.2297, -0.0548,  0.0504, -0.1217,  0.1469,\n",
      "          0.2204, -0.1834,  0.2173,  0.0468,  0.1847,  0.0338,  0.1205, -0.0353],\n",
      "        [ 0.1928,  0.0370, -0.1167,  0.0637, -0.1152, -0.0293, -0.1015,  0.1659,\n",
      "         -0.1974, -0.1152, -0.0706, -0.1503,  0.0236, -0.2470,  0.2257, -0.2124],\n",
      "        [ 0.1931,  0.0417, -0.0812,  0.1545,  0.0389,  0.2019,  0.0274, -0.0786,\n",
      "          0.0671, -0.0677,  0.1052,  0.2232,  0.1445, -0.1094,  0.1442,  0.0448],\n",
      "        [ 0.1270, -0.1523, -0.2475, -0.0966, -0.1918,  0.2051,  0.0720,  0.1036,\n",
      "          0.0791, -0.0043,  0.1956, -0.1776,  0.0157, -0.1707,  0.0771, -0.0861],\n",
      "        [ 0.0767, -0.0520,  0.2074, -0.1482, -0.1491, -0.1491,  0.2249,  0.0834,\n",
      "          0.2405, -0.2063, -0.2480, -0.1956, -0.1682,  0.1012,  0.0895,  0.2077],\n",
      "        [-0.1290, -0.1703,  0.1327, -0.1010,  0.1516, -0.0594,  0.1430, -0.1940,\n",
      "         -0.1262,  0.0764,  0.0528, -0.0638,  0.1490,  0.1698, -0.1814, -0.1334],\n",
      "        [ 0.2289, -0.0844, -0.0886, -0.2419, -0.1432,  0.0625, -0.0330, -0.1815,\n",
      "          0.0059, -0.1708, -0.2121, -0.1377, -0.2188, -0.1592,  0.2499,  0.0472],\n",
      "        [ 0.0770, -0.2332, -0.1642, -0.0832,  0.0391, -0.2199, -0.1077, -0.1498,\n",
      "          0.0007, -0.0931, -0.0173, -0.1694, -0.1716, -0.1458, -0.0855, -0.1973]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Update the weights using the learning rate and the gradients\n",
    "weight0 = weight0 - lr * grads0\n",
    "weight1 = weight1 - lr * grads1\n",
    "weight2 = weight2 - lr * grads2\n",
    "print(weight0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07c71d-4eb5-4b41-aa42-e57747b012b3",
   "metadata": {},
   "source": [
    "### Ex.8 - Using the PyTorch optimizer\n",
    "\n",
    "In the previous exercise, you manually updated the weight of a network. You now know what's going on under the hood, but this approach is not scalable to a network of many layers.\n",
    "\n",
    "Thankfully, the PyTorch SGD optimizer does a similar job in a handful of lines of code. In this exercise, you will practice the last step to complete the training loop: updating the weights using a PyTorch optimizer.\n",
    "\n",
    "A neural network has been created and provided as the model variable. This model was used to run a forward pass and create the tensor of predictions pred. The one-hot encoded tensor is named target and the cross entropy loss function is stored as criterion.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Use optim to create an SGD optimizer with a learning rate of your choice (must be less than one) for the model provided.\n",
    "2. Update the model's parameters using the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10addb79-678c-4706-a423-5eb366fc8a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lr: 0.001\n",
      "Sample: tensor([[-0.3165, -0.3995, -0.4551, -0.5769,  0.2253, -0.8436,  0.6609, -1.3375,\n",
      "         -0.3312,  0.2476, -0.0099,  1.3701,  0.5060, -0.6079,  0.0933, -0.0922]])\n",
      "Target: tensor([[1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.tensor([[\n",
    "    -0.3165, -0.3995, -0.4551, -0.5769,  0.2253, -0.8436,  0.6609, -1.3375,\n",
    "    -0.3312,  0.2476, -0.0099,  1.3701,  0.5060, -0.6079,  0.0933, -0.0922\n",
    "]])\n",
    "\n",
    "target = torch.tensor([[1., 0.]])\n",
    "\n",
    "Lr = 0.001\n",
    "\n",
    "print('Lr:', lr)\n",
    "print('Sample:', sample)\n",
    "print('Target:', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5f9a83c-641f-4611-a95d-e81678f96e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (1): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (2): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "Preds: tensor([[0.2134, 0.2295]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Setting the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.Linear(4, 2)\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Predictions\n",
    "preds = model(sample)\n",
    "print('Preds:', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cdc23e9a-fb33-4b32-bc93-200ead3527c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.7012, dtype=torch.float64, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss and compute the gradients\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(preds.double(), target.double())\n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da6b4ca5-1d5d-4d5e-995e-e961153e243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradients of the loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a8e4de6-5a31-4a37-ad01-d44b662eafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81c79425-84fd-44a0-b150-c20067fde57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec52ad-370c-4c73-a98b-8729a10b4d59",
   "metadata": {},
   "source": [
    "## 2.4 Writing our first training loop\n",
    "\n",
    "### Introducing the Data Science Salary dataset\n",
    "\n",
    "- The target is salary in US dollars; it is not a category but a continuous quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eeb19194-9466-423b-9de9-29ef56b33ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_size</th>\n",
       "      <th>salary_in_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.109780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.137533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.204520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experience_level  employment_type  remote_ratio  company_size  \\\n",
       "0                 0                2           0.5             0   \n",
       "1                 3                2           1.0             0   \n",
       "2                 1                2           0.0             1   \n",
       "3                 1                2           0.5             0   \n",
       "4                 0                2           1.0             2   \n",
       "\n",
       "   salary_in_usd  \n",
       "0       0.102982  \n",
       "1       0.109780  \n",
       "2       0.137533  \n",
       "3       0.380363  \n",
       "4       0.204520  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data-sources/data-salaries.csv')\n",
    "df = df[['experience_level', 'employment_type', 'remote_ratio',\n",
    "         'company_size', 'salary_in_usd']]\n",
    "\n",
    "# Encoding categorical variables\n",
    "df['experience_level'] = df.experience_level.astype('category').cat.codes\n",
    "df['employment_type'] = df.employment_type.astype('category').cat.codes\n",
    "df['company_size'] = df.company_size.astype('category').cat.codes\n",
    "\n",
    "# Normalizing numerical variables\n",
    "df['remote_ratio'] = (df.remote_ratio-df.remote_ratio.min())/(df.remote_ratio.max()-df.remote_ratio.min())\n",
    "df['salary_in_usd'] = (df.salary_in_usd-df.salary_in_usd.min())/(df.salary_in_usd.max()-df.salary_in_usd.min())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a01878-23dc-4bd4-bfe9-9543006a70f2",
   "metadata": {},
   "source": [
    "### Introducing the Mean Squared Error Loss\n",
    "\n",
    "- The mean squared error loss (MSE loss) is the squared difference between the prediction and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ded70f8-c7ca-41ec-9611-c19ee2be71c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.335704905\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_loss(prediction, target):\n",
    "    return np.mean((prediction - target)**2)\n",
    "\n",
    "target = np.array([[1., 0.]])\n",
    "preds = np.array([[0.2134, 0.2295]])\n",
    "\n",
    "# Prediction and target are float tensors\n",
    "loss = mean_squared_loss(preds, target)\n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c89683c8-81c9-4372-bfe1-29c0adf0115b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.3357)\n"
     ]
    }
   ],
   "source": [
    "target = torch.tensor([[1., 0.]])\n",
    "preds = torch.tensor([[0.2134, 0.2295]])\n",
    "\n",
    "# in PyTorch\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Prediction and target are float tensors\n",
    "loss = criterion(preds, target)\n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f21bf0-d05f-431b-87d2-0ac1ae57323e",
   "metadata": {},
   "source": [
    "### Before the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8631c644-fb24-4b80-bd52-cdc12b7bb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into features y target\n",
    "features = df.drop(columns='salary_in_usd')\n",
    "target = df[['salary_in_usd']]\n",
    "\n",
    "# Create the dataset and the dataloader\n",
    "dataset = TensorDataset(torch.tensor(features.values).float(),\n",
    "                        torch.tensor(target.values).float())\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49214c4d-e720-4b80-80be-e875537dc431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Create the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "\n",
    "# Create the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eba360-974a-4ab0-82e5-c230a95bd7dd",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15e66527-1a5a-46f4-8698-59f43504228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the dataset multiple times\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        # Set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get feature and target from the data loader\n",
    "        feature, target = data\n",
    "\n",
    "        # Run a forward pass\n",
    "        pred = model(feature)\n",
    "\n",
    "        # Compute loss and gradients\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f6d4c-0da2-4831-bbc5-87d3513bda71",
   "metadata": {},
   "source": [
    "### Ex.9 - Using the MSELoss\n",
    "\n",
    "Recall that we can't use cross-entropy loss for regression problems. The mean squared error loss (MSELoss) is a common loss function for regression problems. In this exercise, you will practice calculating and observing the loss using NumPy as well as its PyTorch implementation.\n",
    "\n",
    "The torch package has been imported as well as numpy as np and torch.nn as nn.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Calculate the MSELoss using NumPy.\n",
    "2. Create a MSELoss function using PyTorch.\n",
    "3. Convert y_hat and y to tensors and then float data types, and then use them to calculate MSELoss using PyTorch as `mse_pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb35fcbe-c960-4079-ba0e-a48958f9be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.0\n",
      "tensor(81., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Gettin y and y_hat\n",
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y - y_hat)**2)\n",
    "print(mse_numpy)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y).double(),\n",
    "                        torch.tensor(y_hat).double())\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eeff58-50c1-4814-a126-e363eff2dfa5",
   "metadata": {},
   "source": [
    "### Ex.10 - Writing a training loop\n",
    "\n",
    "In scikit-learn, the whole training loop is contained in the .fit() method. In PyTorch, however, you implement the loop manually. While this provides control over loop's content, it requires a custom implementation.\n",
    "\n",
    "You will write a training loop every time you train a deep learning model with PyTorch, which you'll practice in this exercise. The show_results() function provided will display some sample ground truth and the model predictions.\n",
    "\n",
    "The package imports provided are: pandas as pd, torch, torch.nn as nn, torch.optim as optim, as well as DataLoader and TensorDataset from torch.utils.data.\n",
    "\n",
    "The following variables have been created: dataloader, containing the dataloader; model, containing the neural network; criterion, containing the loss function, nn.MSELoss(); optimizer, containing the SGD optimizer; and num_epochs, containing the number of epochs.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "1. Write a for loop that iterates over the dataloader; this should be nested within a for loop that iterates over a range equal to the number of epochs.\n",
    "2. Set the gradients of the optimizer to zero.\n",
    "3. Write the forward pass.\n",
    "4. Compute the MSE loss value using the criterion() function provided.\n",
    "5. Compute the gradients.\n",
    "6. Update the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7320aec6-2be0-48f4-bb7b-281e850288a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_size</th>\n",
       "      <th>salary_in_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.109780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.137533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.204520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experience_level  employment_type  remote_ratio  company_size  \\\n",
       "0                 0                2           0.5             0   \n",
       "1                 3                2           1.0             0   \n",
       "2                 1                2           0.0             1   \n",
       "3                 1                2           0.5             0   \n",
       "4                 0                2           1.0             2   \n",
       "\n",
       "   salary_in_usd  \n",
       "0       0.102982  \n",
       "1       0.109780  \n",
       "2       0.137533  \n",
       "3       0.380363  \n",
       "4       0.204520  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing environment\n",
    "def show_results(model, dataloader):\n",
    "    model.eval()\n",
    "    iter_loader = iter(dataloader)\n",
    "    \n",
    "    for _ in range(3):\n",
    "        feature, target = next(iter_loader)\n",
    "        preds = model(feature)\n",
    "        \n",
    "        for p, t in zip(preds, target):\n",
    "            print(f'Ground truth salary: {t.item():.3f}. Predicted salary: {p.item():.3f}.')\n",
    "\n",
    "# Dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b81acae0-3f3c-4187-a278-572dca258385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into features y target\n",
    "features_data = df.drop(columns='salary_in_usd')\n",
    "target_data = df[['salary_in_usd']]\n",
    "\n",
    "# Create the dataset and the dataloader\n",
    "dataset = TensorDataset(torch.tensor(features_data.values).float(),\n",
    "                        torch.tensor(target_data.values).float())\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Set the num of ephocs\n",
    "num_epochs = 10\n",
    "\n",
    "# Create the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5eecd9e-68bb-4e69-8af6-71069c51e656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d66cdae-da27-4910-bd3b-6dbf72eb372c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth salary: 0.246. Predicted salary: -0.204.\n",
      "Ground truth salary: 0.297. Predicted salary: -0.186.\n",
      "Ground truth salary: 0.051. Predicted salary: -0.189.\n",
      "Ground truth salary: 0.106. Predicted salary: -0.179.\n",
      "Ground truth salary: 0.063. Predicted salary: -0.159.\n",
      "Ground truth salary: 0.075. Predicted salary: -0.197.\n",
      "Ground truth salary: 0.389. Predicted salary: -0.204.\n",
      "Ground truth salary: 0.246. Predicted salary: -0.192.\n",
      "Ground truth salary: 0.022. Predicted salary: -0.139.\n",
      "Ground truth salary: 0.287. Predicted salary: -0.204.\n",
      "Ground truth salary: 0.182. Predicted salary: -0.165.\n",
      "Ground truth salary: 0.157. Predicted salary: -0.197.\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        # Set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run a forward pass\n",
    "        feature, target = data\n",
    "        prediction = model(feature)  \n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(prediction, target) \n",
    "        \n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model's parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "show_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5929a-e5f9-4c03-8e17-3924dc6eaced",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
